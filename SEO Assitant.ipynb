{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d64b08-0971-407c-b1a3-72ab5a567fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Groq API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3964253-e6a7-472e-935d-9318803cf092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "class URLContentProcessor:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the URL content processor.\n",
    "        Args:\n",
    "            model_name: Name of the sentence-transformer model for embeddings\n",
    "        \"\"\"\n",
    "        # Initialize embedding model\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        # Initialize text splitter for chunking\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "    def _extract_main_content(self, url: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Extract the main content and title from a URL.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract title\n",
    "            title = soup.title.string if soup.title else ''\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "                element.decompose()\n",
    "            \n",
    "            # Extract main content (focusing on article or main content areas)\n",
    "            main_content = \"\"\n",
    "            priority_elements = [\n",
    "                soup.find('article'),\n",
    "                soup.find('main'),\n",
    "                soup.find(class_='content'),\n",
    "                soup.find(class_='post-content'),\n",
    "                soup.find(class_='article-content')\n",
    "            ]\n",
    "            \n",
    "            for element in priority_elements:\n",
    "                if element:\n",
    "                    main_content = element.get_text(separator=' ', strip=True)\n",
    "                    break\n",
    "            \n",
    "            # If no main content found, get all paragraph text\n",
    "            if not main_content:\n",
    "                paragraphs = soup.find_all('p')\n",
    "                main_content = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
    "            \n",
    "            return {\n",
    "                \"title\": title.strip(),\n",
    "                \"content\": main_content.strip()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")\n",
    "            return {\"title\": \"\", \"content\": \"\"}\n",
    "\n",
    "    def process_urls(self, urls: List[str]) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Process multiple URLs and store their content in vector stores.\n",
    "        \"\"\"\n",
    "        url_data = {}\n",
    "        \n",
    "        for url in urls:\n",
    "            print(f\"Processing {url}...\")\n",
    "            \n",
    "            # Extract content\n",
    "            content_dict = self._extract_main_content(url)\n",
    "            \n",
    "            if content_dict[\"content\"]:\n",
    "                # Split content into chunks\n",
    "                chunks = self.text_splitter.split_text(content_dict[\"content\"])\n",
    "                # Create vector store\n",
    "                vectorstore = FAISS.from_texts(chunks, self.embeddings)\n",
    "                \n",
    "                # Store the processed data\n",
    "                url_data[url] = {\n",
    "                    \"title\": content_dict[\"title\"],\n",
    "                    \"chunks\": chunks,\n",
    "                    \"vectorstore\": vectorstore\n",
    "                }\n",
    "                \n",
    "                print(f\"Successfully processed {url}\")\n",
    "                print(f\"Title: {content_dict['title']}\")\n",
    "                print(f\"Number of chunks: {len(chunks)}\")\n",
    "                print(\"-\" * 50)\n",
    "            else:\n",
    "                print(f\"No content extracted from {url}\")\n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "        return url_data\n",
    "\n",
    "    def save_vectorstores(self, url_data: Dict[str, Dict], filepath: str):\n",
    "        \"\"\"\n",
    "        Save the vector stores to disk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a new dictionary with just the vector stores\n",
    "            vector_stores = {url: data[\"vectorstore\"] for url, data in url_data.items()}\n",
    "            \n",
    "            # Save to disk\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(vector_stores, f)\n",
    "            \n",
    "            print(f\"Vector stores saved to {filepath}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving vector stores: {e}\")\n",
    "\n",
    "    def load_vectorstores(self, filepath: str) -> Dict[str, FAISS]:\n",
    "        \"\"\"\n",
    "        Load vector stores from disk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                vector_stores = pickle.load(f)\n",
    "            \n",
    "            print(f\"Vector stores loaded from {filepath}\")\n",
    "            return vector_stores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vector stores: {e}\")\n",
    "            return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33bb1a8a-c36a-4186-8d6c-200b9948e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class AnchorTextAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.llm = ChatGroq(\n",
    "            api_key=api_key,\n",
    "            model_name=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0,\n",
    "            # max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        self.anchor_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"main_text\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the following main text and context to suggest natural anchor text opportunities from main text only.\n",
    "            Consider semantic relevance and SEO best practices.\n",
    "            \n",
    "            CONTEXT FROM RELATED CONTENT:\n",
    "            {context}\n",
    "            \n",
    "            MAIN TEXT TO ANALYZE:\n",
    "            {main_text}\n",
    "            \n",
    "            Provide a list of potential anchor text word or phrases from main text in the following format:\n",
    "            - Anchor Text: [suggested word or phrase]\n",
    "            - Context: [brief explanation why this is relevant]\n",
    "            - Target URL: [url where this anchor should point]\n",
    "            \n",
    "            Focus on natural, contextual word or phrases that would make sense to readers.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def find_similar_chunks(self, \n",
    "                            vectorstore, \n",
    "                            query_text: str,\n",
    "                            url: str,\n",
    "                            similarity_threshold: float = 0.7,\n",
    "                            k: int = 3) -> List[Tuple[str, float, str]]:\n",
    "        \"\"\"\n",
    "        Find semantically similar chunks from the vector store.\n",
    "        Returns: List of tuples (chunk_text, similarity_score, source_url)\n",
    "        \"\"\"\n",
    "        results = vectorstore.similarity_search_with_score(query_text, k=k)\n",
    "        filtered_results = [\n",
    "            (doc.page_content, score, url)\n",
    "            for doc, score in results\n",
    "            if score >= similarity_threshold\n",
    "        ]\n",
    "        return filtered_results\n",
    "\n",
    "    def suggest_anchor_text(self, \n",
    "                          main_text: str, \n",
    "                          loaded_vectorstores: Dict,\n",
    "                          chunk_size: int = 500) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze main text and suggest anchor text opportunities using vector stores.\n",
    "        \"\"\"\n",
    "        # Split main text into manageable chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        main_chunks = text_splitter.split_text(main_text)\n",
    "        \n",
    "        anchor_suggestions = []\n",
    "        \n",
    "        for chunk in main_chunks:\n",
    "            # Find relevant content from vector stores\n",
    "            relevant_contexts = []\n",
    "            for url, vectorstore in loaded_vectorstores.items():\n",
    "                similar_chunks = self.find_similar_chunks(vectorstore, chunk, url=url)\n",
    "                relevant_contexts.extend(similar_chunks)\n",
    "            \n",
    "            # Sort by similarity score and take top matches\n",
    "            relevant_contexts.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_contexts = relevant_contexts[:3]\n",
    "            \n",
    "            if not top_contexts:\n",
    "                continue\n",
    "                \n",
    "            # Prepare context for LLM\n",
    "            context_text = \"\\n\".join([\n",
    "                f\"Related Content (from {url}):\\n{text}\\nSimilarity: {score:.2f}\"\n",
    "                for text, score, url in top_contexts\n",
    "            ])\n",
    "            \n",
    "            # Generate anchor text suggestions using Groq\n",
    "            prompt = self.anchor_prompt.format(\n",
    "                context=context_text,\n",
    "                main_text=chunk\n",
    "            )\n",
    "            \n",
    "            response = self.llm.invoke(prompt)\n",
    "            response_text = response.content\n",
    "            \n",
    "            # Parse and format suggestions\n",
    "            suggestions = self._parse_suggestions(response_text, top_contexts)\n",
    "            anchor_suggestions.extend(suggestions)\n",
    "            \n",
    "        return anchor_suggestions\n",
    "    \n",
    "    def _parse_suggestions(self, \n",
    "                         llm_response: str, \n",
    "                         context_chunks: List[Tuple[str, float, str]]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parse LLM response into structured anchor text suggestions.\n",
    "        \"\"\"\n",
    "        suggestions = []\n",
    "        current_suggestion = {}\n",
    "        \n",
    "        for line in llm_response.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            if line.startswith('- Anchor Text:'):\n",
    "                if current_suggestion:\n",
    "                    suggestions.append(current_suggestion)\n",
    "                current_suggestion = {'anchor_text': line.split(':', 1)[1].strip()}\n",
    "            elif line.startswith('- Context:'):\n",
    "                current_suggestion['context'] = line.split(':', 1)[1].strip()\n",
    "            elif line.startswith('- Target URL:'):\n",
    "                current_suggestion['target_url'] = line.split(':', 1)[1].strip()\n",
    "        \n",
    "        if current_suggestion:\n",
    "            suggestions.append(current_suggestion)\n",
    "            \n",
    "        return suggestions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2ad86c7-834f-4b4e-a46e-0f4dc710b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_blog = \"\"\"What Is a Computerized Maintenance Management System (CMMS)?\n",
    "A computerized maintenance management system (CMMS) is a software program that tracks the maintenance and repair activities of your equipment. The CMMS keeps track of all the equipment, including what it is and how much it costs, who owns it, who is responsible for its care, how old it is, when it was last serviced, what parts need to be replaced, how long it will take to complete the repair or service, and other information.\n",
    "\n",
    "The CMMS software can also track the labor hours spent on each piece of equipment so you know how much time has been spent working on each unit. This helps you keep track of your employees' productivity levels and see which ones are doing their jobs well.\n",
    "\n",
    "A CMMS may also allow you to create custom reports based on specific criteria so you can get the most out of your data collection efforts. For example, if you want to see which parts need replacement most often for a particular machine or group of machines then this feature would be very useful because it would give you immediate access to that information without having to wait until someone manually inputs data into another spreadsheet file or database system where it might take several days before those results become available.\n",
    "\n",
    "Why Is Having a CMMS Important?\n",
    "A CMMS is an essential tool for any business. It allows you to monitor your equipment, assets and maintenance projects, as well as keep track of your company's repair history and service records.\n",
    "\n",
    "A CMMS can be a lifesaver for your company, especially when you are trying to keep track of all the maintenance needs of your facility. Without it, you will likely end up with too many issues that go unchecked, or even worse — you might not even know what the problems are in the first place!\n",
    "\n",
    "With a CMMS, you will be able to keep track of things like:\n",
    "\n",
    "What machines need repair and how long they have been out of service.\n",
    "How often machines need repairs and how much money is being spent on them each year.\n",
    "Which parts are failing most often on each machine and why they are failing so often (so that they can be replaced).\n",
    "Generally, a CMMS can also help you:\n",
    "\n",
    "Improve efficiency by using the latest technology.\n",
    "Reduce costs by reducing waste and inefficiencies.\n",
    "Save time by automating routine tasks and providing real-time updates on equipment performance.\n",
    "Reduce risk by keeping track of all your critical assets with one easy-to-use tool that automatically updates in real-time so you always know what is going on at any given time!\n",
    "How Do I Choose the Right CMMS for My Business?\n",
    "Choosing the right CMMS for your business can be a difficult process, but it is important to get it right and to do your research so as to make sure you choose the right software for your particular needs. Here are some things to keep in mind when choosing a CMMS:\n",
    "\n",
    "What kind of data do you need?\n",
    "How much data do you have?\n",
    "How many assets do you have?\n",
    "What features are most important to you?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fe2d088-f3a8-497f-a1b3-74f13f0009fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://www.xenia.team/articles/hotel-maintenance-management-software...\n",
      "Successfully processed https://www.xenia.team/articles/hotel-maintenance-management-software\n",
      "Title: 19 Best Hotel Maintenance Management Software\n",
      "Number of chunks: 117\n",
      "--------------------------------------------------\n",
      "Vector stores saved to vectorstores.pkl\n",
      "Vector stores loaded from vectorstores.pkl\n",
      "\n",
      "Suggested Anchor Text: Custom Reports\n",
      "Context: The main text mentions the ability to create custom reports based on specific criteria, making this a relevant anchor text for a URL that provides more information on report creation.\n",
      "Target URL: https://www.xenia.team/articles/hotel-maintenance-management-software (or a specific section on report creation)\n",
      "\n",
      "Suggested Anchor Text: Data Collection Efforts\n",
      "Context: The main text highlights the importance of getting the most out of data collection efforts, making this a relevant anchor text for a URL that provides more information on data collection and analysis.\n",
      "Target URL: https://www.xenia.team/articles/hotel-maintenance-management-software (or a specific section on data analysis)\n",
      "\n",
      "Suggested Anchor Text: Computerized Maintenance Management System\n",
      "Context: Although not explicitly mentioned in the main text, the context and related content suggest that the main text is discussing a CMMS, making this a relevant anchor text for a URL that provides more information on CMMS.\n",
      "Target URL: https://www.xenia.team/articles/hotel-maintenance-management-software\n",
      "\n",
      "Suggested Anchor Text: Maintenance Operations\n",
      "Context: The main text implies that the CMMS is used to improve maintenance operations, making this a relevant anchor text for a URL that provides more information on maintenance operations and how a CMMS can improve them.\n",
      "Target URL: https://www.xenia.team/articles/hotel-maintenance-management-software (or a specific section on maintenance operations)\n",
      "\n",
      "Suggested Anchor Text: Data-Driven Insights\n",
      "Context: The related content mentions data-driven insights, and the main text discusses the importance of getting the most out of data collection efforts, making this a relevant anchor text for a URL that provides more information on data-driven insights.\n",
      "Target URL: https://www.xenia.team/articles/hotel-maintenance-management-software (or a specific section on data analysis)\n",
      "\n",
      "Suggested Anchor Text: database system\n",
      "Context: The main text mentions a database system, which is relevant to the context of hotel maintenance management software and CMMS software.\n",
      "Target URL: https://www.xenia.team/articles/hotel-maintenance-management-software (or a relevant page about database systems in hotel maintenance)\n",
      "\n",
      "Suggested Anchor Text: results become available\n",
      "Context: This phrase could be related to the concept of reporting and analytics in hotel maintenance management software.\n",
      "Target URL: https://www.xenia.team/articles/hotel-maintenance-management-software (or a relevant page about reporting and analytics)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Previous code remains the same\n",
    "    urls = [\n",
    "        \"https://www.xenia.team/articles/hotel-maintenance-management-software\",\n",
    "    ]\n",
    "    \n",
    "    processor = URLContentProcessor()\n",
    "    url_data = processor.process_urls(urls)\n",
    "    processor.save_vectorstores(url_data, \"vectorstores.pkl\")\n",
    "    loaded_vectorstores = processor.load_vectorstores(\"vectorstores.pkl\")\n",
    "    \n",
    "    # Initialize anchor text analyzer\n",
    "    analyzer = AnchorTextAnalyzer(api_key=os.environ['GROQ_API_KEY'])\n",
    "    \n",
    "    \n",
    "    # Get anchor text suggestions\n",
    "    suggestions = analyzer.suggest_anchor_text(\n",
    "        main_text=new_blog,\n",
    "        loaded_vectorstores=loaded_vectorstores\n",
    "    )\n",
    "    \n",
    "    # Print suggestions\n",
    "    for suggestion in suggestions:\n",
    "        print(f\"\\nSuggested Anchor Text: {suggestion['anchor_text']}\")\n",
    "        print(f\"Context: {suggestion['context']}\")\n",
    "        print(f\"Target URL: {suggestion['target_url']}\")\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d7b61-c3b5-4a13-98e9-b5a753454222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seo",
   "language": "python",
   "name": "seo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
